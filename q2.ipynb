{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - **Author**: Alex Gao\n",
    "# - **Date**: 2024-08-04\n",
    "# - **Description**: This notebook is a part of Datathon 2024. The model used in this notebook is KAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# def merge_and_filter_csv_files(directory):\n",
    "#     # List to hold dataframes\n",
    "#     dataframes = []\n",
    "    \n",
    "#     # Iterate over all files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith('.csv'):\n",
    "#             file_path = os.path.join(directory, filename)\n",
    "#             df = pd.read_csv(file_path)\n",
    "#             dataframes.append(df)\n",
    "    \n",
    "#     # Concatenate all dataframes\n",
    "#     merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "#     # Filter rows based on \"Description\" column containing specific strings\n",
    "#     descriptions_to_keep = [\n",
    "#         \"All industry\", \n",
    "#         \"Food\",\n",
    "#         \"GDP\"\n",
    "#     ]\n",
    "    \n",
    "#     # Filter rows\n",
    "#     filtered_df = merged_df[merged_df[\"Description\"].str.contains('|'.join(descriptions_to_keep)) | (merged_df.index == 0)]\n",
    "    \n",
    "#     # Sort the DataFrame according to \"GeoName\" with \"United States\" on top\n",
    "#     filtered_df[\"GeoName\"] = filtered_df[\"GeoName\"].fillna(\"\")\n",
    "#     sorted_df = filtered_df.sort_values(by=[\"GeoName\", \"TableName\"])\n",
    "#     sorted_df = sorted_df.sort_values(by=[\"GeoName\"], key=lambda x: x.str.contains(\"United States\"), ascending=False)\n",
    "    \n",
    "#     return sorted_df\n",
    "\n",
    "# # Directory where the CSV files are located\n",
    "# directory = './data/SAGDP'\n",
    "\n",
    "# # Merge, filter, and sort the DataFrame\n",
    "# result_df = merge_and_filter_csv_files(directory)\n",
    "\n",
    "# result_df.to_csv('SAGDP_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do changes in employment rates correlate with processed food consumption?\n",
    "\n",
    "# consumed = (percentage of processed meat) * (1 - percent waste) * (produced - stored)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "meat_stats_cold_storage = pd.read_csv(\"Datathon Data/Meat_Stats_Cold_Storage.csv\")\n",
    "\n",
    "meat_stats_meat_production = pd.read_csv(\"Datathon Data/Meat_Stats_Meat_Production.csv\")\n",
    "egg_rows = meat_stats_cold_storage['Animal'] == 'Frozen Eggs'\n",
    "meat_stats_cold_storage_clean = meat_stats_cold_storage[~egg_rows]\n",
    "incomplete_date_cold_storage = meat_stats_cold_storage_clean[meat_stats_cold_storage_clean['Weight'].isna()]\n",
    "\n",
    "cold_storage_dates_to_remove = incomplete_date_cold_storage['Date'].unique()\n",
    "\n",
    "meat_stats_cold_storage_clean = meat_stats_cold_storage_clean[~meat_stats_cold_storage_clean['Date'].isin(cold_storage_dates_to_remove)]\n",
    "\n",
    "meat_stats_cold_storage_clean = meat_stats_cold_storage_clean.drop(columns=['Year', 'Month'])\n",
    "meat_stats_cold_storage_clean = meat_stats_cold_storage_clean.groupby('Date')['Weight'].sum()\n",
    "incomplete_date_meat_production = meat_stats_meat_production[meat_stats_meat_production['Production'].isna()]\n",
    "\n",
    "meat_production_dates_to_remove = incomplete_date_meat_production['Date'].unique()\n",
    "\n",
    "meat_stats_meat_production_clean = meat_stats_meat_production[~meat_stats_meat_production['Date'].isin(meat_production_dates_to_remove)]\n",
    "\n",
    "meat_stats_meat_production_clean = meat_stats_meat_production_clean.drop(columns=['Year', 'Month'])\n",
    "meat_stats_meat_production_clean['Production'] = meat_stats_meat_production_clean['Production'].str.replace(',', '').astype(float)\n",
    "meat_stats_meat_production_clean = meat_stats_meat_production_clean.groupby('Date')['Production'].sum()\n",
    "df_cold_storage = meat_stats_cold_storage_clean.reset_index()\n",
    "df_cold_storage.columns = ['Date', 'Weight']\n",
    "\n",
    "df_meat_production = meat_stats_meat_production_clean.reset_index()\n",
    "df_meat_production.columns = ['Date', 'Production']\n",
    "\n",
    "cold_storage_dates = set(df_cold_storage['Date'])\n",
    "meat_production_dates = set(df_meat_production['Date'])\n",
    "common_dates = cold_storage_dates.intersection(meat_production_dates)\n",
    "\n",
    "filtered_df_cold_storage = df_cold_storage[df_cold_storage['Date'].isin(common_dates)]\n",
    "filtered_df_meat_production = df_meat_production[df_meat_production['Date'].isin(common_dates)]\n",
    "# We will assume there is 35% loss for the food\n",
    "\n",
    "# So, consumed = (1 - 0.35) * (produced - stored)\n",
    "\n",
    "# or consumed = 0.65*(produced - stored)\n",
    "merged_df = pd.merge(filtered_df_cold_storage, filtered_df_meat_production, on='Date')\n",
    "\n",
    "merged_df['Consumption'] = (merged_df['Production'] - merged_df['Weight'])*0.65\n",
    "\n",
    "consumption_df = merged_df[['Date', 'Consumption']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_df['Date'] = pd.to_datetime(consumption_df['Date'], format='%b-%Y')\n",
    "\n",
    "# Format the 'Date' column to \"Y-M\"\n",
    "consumption_df['Date'] = consumption_df['Date'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodities_data = pd.read_csv('./data/all_commodities.csv')\n",
    "stocks_etfs_data = pd.read_csv('./data/all_stock_and_etfs.csv')\n",
    "exchange_rate_data = pd.read_csv('./data/ExchangeRate.csv')\n",
    "cold_storage_data = pd.read_csv('./data/Meat_Stats_Cold_Storage.csv')\n",
    "meat_production_data = pd.read_csv('./data/Meat_Stats_Meat_Production.csv')\n",
    "stock_descriptions_data = pd.read_csv('./data/stock_descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodities_data = commodities_data.drop(columns=['Unit'])\n",
    "\n",
    "# Pivot the DataFrame to have commodities as columns\n",
    "commodities_data_pivot = commodities_data.pivot(index='Date-Time', columns='Commodity', values='Value')\n",
    "\n",
    "# Flatten the columns (if necessary, depends on pivot result)\n",
    "commodities_data_pivot.columns = [f\"{col}_Value\" for col in commodities_data_pivot.columns]\n",
    "\n",
    "# Reset the index to make 'Date-Time' a column again\n",
    "commodities_data_pivot = commodities_data_pivot.reset_index()\n",
    "\n",
    "commodities_data_pivot['Date-Time'] = pd.to_datetime(commodities_data_pivot['Date-Time'], format='%Y-%m-%d')\n",
    "\n",
    "commodities_data_pivot['Date-Time'] = commodities_data_pivot['Date-Time'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_etfs_data['Date-Time'] = pd.to_datetime(stocks_etfs_data['Date-Time'])\n",
    "stocks_etfs_data['Date-Time'] = stocks_etfs_data['Date-Time'].dt.strftime('%Y-%m')\n",
    "grouped_stock = stocks_etfs_data.groupby(['Date-Time', 'Ticker_Symbol']).mean().reset_index()\n",
    "\n",
    "grouped_stock_flat = grouped_stock.pivot_table(\n",
    "    index='Date-Time', \n",
    "    columns='Ticker_Symbol', \n",
    "    values=['Open', 'High', 'Low', 'Close', 'Volume'], \n",
    "    aggfunc='mean'\n",
    ")\n",
    "grouped_stock_flat.columns = [f'{col[1]}_{col[0]}' for col in grouped_stock_flat.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## and only take data from 1999-11 to 2024-02\n",
    "\n",
    "# commodities_data_pivot\n",
    "# grouped_stock_flat\n",
    "# consumption_df\n",
    "\n",
    "## these are the dataframes that we will merge with, basically merge the others into grouped_stock_flat\n",
    "\n",
    "merged_df = pd.merge(grouped_stock_flat, commodities_data_pivot, on='Date-Time', how='outer')\n",
    "\n",
    "merged_df = pd.merge(merged_df, consumption_df, left_on='Date-Time', right_on='Date', how='outer')\n",
    "\n",
    "merged_df = merged_df[(merged_df['Date-Time'] >= '1999-11') & (merged_df['Date-Time'] <= '2024-02')]\n",
    "desired_substrings = ['DIA', 'ONEQ', 'VOO', 'SPY']\n",
    "\n",
    "# Step 2: Filter the columns that contain any of the desired substrings\n",
    "selected_columns = [col for col in merged_df.columns if any(sub in col for sub in desired_substrings)]\n",
    "## drop column \"Date\"\n",
    "merged_df.drop(columns=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize it, but merged_df first column is Date-Time, so we will normalize from the second column\n",
    "## but stil keep the Date-Time column\n",
    "\n",
    "\n",
    "mean = merged_df.iloc[:, 1:].mean()\n",
    "std = merged_df.iloc[:, 1:].std()\n",
    "\n",
    "## how to normalize merged_df?\n",
    "\n",
    "merged_df.iloc[:, 1:] = (merged_df.iloc[:, 1:] - mean) / std\n",
    "merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## x_columns = columns except selected_columns\n",
    "\n",
    "x_columns = [col for col in merged_df.columns if col not in selected_columns]\n",
    "\n",
    "## discard \"Date-Time\" column\n",
    "\n",
    "x_columns = x_columns[1:]\n",
    "\n",
    "\n",
    "merged_df ['DIA_mean'] = merged_df[['DIA_Open', 'DIA_Close']].mean(axis=1)\n",
    "merged_df ['ONEQ_mean'] = merged_df[['ONEQ_Open', 'ONEQ_Close']].mean(axis=1)\n",
    "merged_df ['SPY_mean'] = merged_df[['SPY_Open', 'SPY_Close']].mean(axis=1)\n",
    "merged_df ['VOO_mean'] = merged_df[['VOO_Open', 'VOO_Close']].mean(axis=1)\n",
    "\n",
    "target_columns = [\n",
    "    'DIA_mean','SPY_mean', 'VOO_mean'\n",
    "]\n",
    "\n",
    "# df_x = merged_df[x_columns]\n",
    "# df_y = merged_df[target_columns]\n",
    "# df_y = df_y.iloc[1:]\n",
    "# df_x = df_x.iloc[:-1]\n",
    "\n",
    "df_before2022 = merged_df[merged_df['Date-Time'] < '2022-01']\n",
    "\n",
    "df_after2022 = merged_df[merged_df['Date-Time'] >= '2022-01']\n",
    "\n",
    "df_x_train = df_before2022[x_columns]\n",
    "df_y_train = df_before2022[target_columns]\n",
    "df_x_val = df_after2022[x_columns]\n",
    "df_y_val = df_after2022[target_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the last column of df_x_train and df_x_val\n",
    "## drop the first column of df_y_train and df_y_val\n",
    "\n",
    "df_x_train = df_x_train.iloc[:-1]\n",
    "df_x_val = df_x_val.iloc[:-1]\n",
    "df_y_train = df_y_train.iloc[1:]\n",
    "df_y_val = df_y_val.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finally fill na with 0 again\n",
    "\n",
    "df_x_train.fillna(0, inplace=True)\n",
    "df_x_val.fillna(0, inplace=True)\n",
    "df_y_train.fillna(0, inplace=True)\n",
    "df_y_val.fillna(0, inplace=True)\n",
    "\n",
    "x_tensor_train = torch.tensor(df_x_train.values, dtype=torch.float32)\n",
    "y_tensor_train = torch.tensor(df_y_train.values, dtype=torch.float32)\n",
    "x_tensor_val = torch.tensor(df_x_val.values, dtype=torch.float32)\n",
    "y_tensor_val = torch.tensor(df_y_val.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SplineLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, init_scale: float = 0.1, **kw) -> None:\n",
    "        self.init_scale = init_scale\n",
    "        super().__init__(in_features, out_features, bias=False, **kw)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.trunc_normal_(self.weight, mean=0, std=self.init_scale)\n",
    "\n",
    "class RadialBasisFunction(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_min: float = -2.,\n",
    "        grid_max: float = 2.,\n",
    "        num_grids: int = 8,\n",
    "        denominator: float = None,  # larger denominators lead to smoother basis\n",
    "    ):\n",
    "        super().__init__()\n",
    "        grid = torch.linspace(grid_min, grid_max, num_grids)\n",
    "        self.grid = torch.nn.Parameter(grid, requires_grad=False)\n",
    "        self.denominator = denominator or (grid_max - grid_min) / (num_grids - 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-((x[..., None] - self.grid) / self.denominator) ** 2)\n",
    "\n",
    "class FastKANLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        grid_min: float = -2.,\n",
    "        grid_max: float = 2.,\n",
    "        num_grids: int = 8,\n",
    "        use_base_update: bool = True,\n",
    "        base_activation = nn.SiLU(),\n",
    "        spline_weight_init_scale: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layernorm = nn.LayerNorm(input_dim)\n",
    "        self.rbf = RadialBasisFunction(grid_min, grid_max, num_grids)\n",
    "        self.spline_linear = SplineLinear(input_dim * num_grids, output_dim, spline_weight_init_scale)\n",
    "        self.use_base_update = use_base_update\n",
    "        if use_base_update:\n",
    "            self.base_activation = base_activation\n",
    "            self.base_linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, time_benchmark=False):\n",
    "        if not time_benchmark:\n",
    "            spline_basis = self.rbf(self.layernorm(x))\n",
    "        else:\n",
    "            spline_basis = self.rbf(x)\n",
    "        ret = self.spline_linear(spline_basis.view(*spline_basis.shape[:-2], -1))\n",
    "        if self.use_base_update:\n",
    "            base = self.base_linear(self.base_activation(x))\n",
    "            ret = ret + base\n",
    "        return ret\n",
    "\n",
    "class KAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_hidden: list,\n",
    "        grid_min: float = -2.,\n",
    "        grid_max: float = 2.,\n",
    "        num_grids: int = 8,\n",
    "        use_base_update: bool = True,\n",
    "        base_activation = nn.SiLU(),\n",
    "        spline_weight_init_scale: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            FastKANLayer(\n",
    "                in_dim, out_dim,\n",
    "                grid_min=grid_min,\n",
    "                grid_max=grid_max,\n",
    "                num_grids=num_grids,\n",
    "                use_base_update=use_base_update,\n",
    "                base_activation=base_activation,\n",
    "                spline_weight_init_scale=spline_weight_init_scale,\n",
    "            ) for in_dim, out_dim in zip(layers_hidden[:-1], layers_hidden[1:])\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def evaluate(model, val_features, val_labels, batch_size=3, device='cuda'):\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        num_samples = val_features.shape[0]\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            batch_features = val_features[start_idx:end_idx].to(device)\n",
    "            batch_labels = val_labels[start_idx:end_idx].to(device)\n",
    "\n",
    "            outputs = model(batch_features)\n",
    "            val_preds.append(outputs.detach().cpu().numpy().flatten())\n",
    "            val_labels_list.append(batch_labels.detach().cpu().numpy().flatten())\n",
    "\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_labels_list = np.concatenate(val_labels_list)\n",
    "\n",
    "    # Calculate IC for all data\n",
    "    val_ic_all, _ = pearsonr(val_preds, val_labels_list)\n",
    "\n",
    "    # Calculate IC for |y| > 5\n",
    "    mask_5 = np.abs(val_labels_list) > 0.5\n",
    "    val_ic_gt_5, _ = pearsonr(val_preds[mask_5], val_labels_list[mask_5])\n",
    "\n",
    "    # Calculate IC for |y| > 9\n",
    "    mask_9 = np.abs(val_labels_list) > 0.9\n",
    "    val_ic_gt_9, _ = pearsonr(val_preds[mask_9], val_labels_list[mask_9])\n",
    "\n",
    "    # Calculate IC for |y| > 12\n",
    "    mask_12 = np.abs(val_labels_list) > 1.2\n",
    "    val_ic_gt_12, _ = pearsonr(val_preds[mask_12], val_labels_list[mask_12])\n",
    "\n",
    "    # Clean up memory\n",
    "    del val_features\n",
    "    del val_labels\n",
    "    del outputs\n",
    "\n",
    "    # Return ICs\n",
    "    return {\n",
    "        'val_ic_all': val_ic_all,\n",
    "        'val_ic_gt_5': val_ic_gt_5,\n",
    "        'val_ic_gt_9': val_ic_gt_9,\n",
    "        'val_ic_gt_12': val_ic_gt_12\n",
    "    }\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, num_epochs=50, batch_size=3, learning_rate=1e-3, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    ics = []\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_features.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_metrics = evaluate(model, x_val, y_val, batch_size, device)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, '\n",
    "              f'Val IC All: {val_metrics[\"val_ic_all\"]:.4f}, '\n",
    "              f'Val IC > 5: {val_metrics[\"val_ic_gt_5\"]:.4f}, '\n",
    "              f'Val IC > 9: {val_metrics[\"val_ic_gt_9\"]:.4f}, '\n",
    "              f'Val IC > 12: {val_metrics[\"val_ic_gt_12\"]:.4f}')\n",
    "        ics.append(val_metrics['val_ic_all'])\n",
    "        losses.append(epoch_loss)\n",
    "    return ics, losses\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_train, y_train, x_val, y_val are already defined\n",
    "\n",
    "ics = []\n",
    "losses = []\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model1 = KAN([x_tensor_train.shape[1], 64, 32, 16, y_tensor_train.shape[1]],\n",
    "             use_base_update=True, base_activation=F.gelu)\n",
    "ics, losses =train_model(model1, x_tensor_train, y_tensor_train, x_tensor_val, y_tensor_val, num_epochs=100,\n",
    "            batch_size=10, learning_rate=1e-3, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot ics and losses\n",
    "\n",
    "plt.plot(ics, label='IC')\n",
    "plt.plot(losses, label='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming model1 is your trained model\n",
    "# and x_tensor_val, y_tensor_val are your validation data\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred = model1(x_tensor_val).detach().cpu().numpy()\n",
    "y_true = y_tensor_val.detach().cpu().numpy()\n",
    "\n",
    "# Plot predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs Actual Values')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = np.abs(model1.layers[0].spline_linear.weight.detach().cpu().numpy()).mean(axis=0)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap of key variables\n",
    "key_vars = ['DIA_mean', 'SPY_mean', 'VOO_mean', 'Consumption', 'Corn_Value']\n",
    "corr_matrix = merged_df[key_vars].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Key Variables')\n",
    "plt.show()\n",
    "\n",
    "# Time series plot of actual vs predicted values for one ETF\n",
    "etf_name = 'DIA_mean'\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_df['Date-Time'][-len(y_true):], y_true[:, 0], label='Actual')\n",
    "plt.plot(merged_df['Date-Time'][-len(y_pred):], y_pred[:, 0], label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(etf_name)\n",
    "plt.title(f'Actual vs Predicted Values for {etf_name}')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
